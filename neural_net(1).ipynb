{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resources:\n",
    "* https://youtu.be/aircAruvnKk\n",
    "* http://neuralnetworksanddeeplearning.com/\n",
    "* playground.tensorflow.org\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "TASK 1\n",
    "INSTRUCTIONS:\n",
    "There are 11 TODOS in this python file\n",
    "Fill each one of those appropriately and you will have a working neural network\n",
    "Instructions and resources have been provided wherever possible.\n",
    "The implementation may not be perfect, so feel free to point out any mistakes / ask any doubts\n",
    "After completing the task, some of the things you could try are (optional):\n",
    "* Implement different cost functions (binary cross-entropy)\n",
    "* Implement different activation functions (tanh, ReLU, softmax)\n",
    "* Incorporate these changes in the neural netwok code so that you can select the loss / activation function\n",
    "* Play with the hyper-paramters!\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "TASK 2\n",
    "INSTRUCTIONS:\n",
    "* Go through the documentaation of scikit from:\n",
    "  https://scikit-image.org/docs/stable/\n",
    "  focus more on the neural network modules\n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html (Neural Network Classifier)\n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html  (Neural Network Regressor)\n",
    "* Go through the MNIST dataset given here:\n",
    "  http://yann.lecun.com/exdb/mnist/\n",
    "  It can also be downloaded directly using scikit:\n",
    "  https://scikit-learn.org/0.19/datasets/mldata.html\n",
    "  But this seems to be deprecated, you could use a workaround given here:\n",
    "  https://stackoverflow.com/questions/47324921/cant-load-mnist-original-dataset-using-sklearn\n",
    "* Build a simple neural network (using scikit) and train it to recognize handwritten digits using the MNIST datasetself.\n",
    "  Make sure that you are able to vsualize the different aspects of the network, play around with the hyper-parameters and\n",
    "  try to get the best possible accuracy and report your accuracy on the ML-SIG group / channel\n",
    "  Remember to test different hyper-parameters on the validation set and to report the accuracy from the test set\n",
    "  https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "\"\"\"\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from IPython import display\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Other Common activation functions are:\n",
    "* tanh\n",
    "* ReLU\n",
    "* Softmax\n",
    "Read more about these at:\n",
    "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\"\"\"\n",
    "\n",
    "def activation(z, derivative=False):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function:\n",
    "    It handles two modes: normal and derivative mode.\n",
    "    Applies a pointwise operation on vectors\n",
    "    Parameters:\n",
    "    ---\n",
    "    z: pre-activation vector at layer l\n",
    "        shape (n[l], batch_size)\n",
    "    Returns:\n",
    "    pontwize activation on each element of the input z\n",
    "    \"\"\"\n",
    "    if derivative:\n",
    "        pass\n",
    "    #todo1-done\n",
    "        return activation(z) * (1 - activation(z))\n",
    "    else:\n",
    "        pass\n",
    "    #todo2-done\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "def cost_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Mean Square Error between a ground truth vector and a prediction vector\n",
    "    Parameters:\n",
    "    ---\n",
    "    y_true: ground-truth vector\n",
    "    y_pred: prediction vector\n",
    "    Returns:\n",
    "    ---\n",
    "    cost: a scalar value representing the loss\n",
    "    \"\"\"\n",
    "   \n",
    "    y_pred= y_pred.T\n",
    "    n = y_pred.shape[1]\n",
    "    #edit\n",
    "    cost = (1./(2*n)) * np.sum((y_true - y_pred) ** 2)\n",
    "    return cost\n",
    "\n",
    "def cost_function_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the loss function w.r.t the activation of the output layer\n",
    "    Parameters:\n",
    "    ---\n",
    "    y_true: ground-truth vector\n",
    "    y_pred: prediction vector\n",
    "    Returns:\n",
    "    ---\n",
    "    cost_prime: derivative of the loss w.r.t. the activation of the output\n",
    "    shape: (n[L], batch_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    if y_true.shape!=y_pred.shape:\n",
    "                y_pred=y_pred.T\n",
    "    loss=(y_true - y_pred) ** 2\n",
    "    ##so just the derivative\n",
    "    #todo3-done\n",
    "    cost_prime=y_pred - y_true\n",
    "    return cost_prime.T\n",
    "#edit\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    This is a custom neural netwok package built from scratch with numpy.\n",
    "    The Neural Network as well as its parameters and training method and procedure will\n",
    "    reside in this class.\n",
    "    Parameters\n",
    "    ---\n",
    "    size: list of number of neurons per layer\n",
    "    Examples\n",
    "    ---\n",
    "    >>> import NeuralNetwork\n",
    "    >>> nn = NeuralNetwork([2, 3, 4, 1])\n",
    "    This means :\n",
    "    1 input layer with 2 neurons\n",
    "    1 hidden layer with 3 neurons\n",
    "    1 hidden layer with 4 neurons\n",
    "    1 output layer with 1 neuron\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size, seed=42):\n",
    "        '''\n",
    "        Instantiate the weights and biases of the network\n",
    "        weights and biases are attributes of the NeuralNetwork class\n",
    "        They are updated during the training\n",
    "        '''\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.size = size\n",
    "        # biases are initialized randomly\n",
    "        self.biases = [np.random.rand(n, 1) for n in self.size[1:]]\n",
    "\n",
    "        # TODO\n",
    "        # initialize the weights randomly\n",
    "        \"\"\"\n",
    "        Be careful with the dimensions of the weights\n",
    "        The dimensions of the weight of any particular layer will depend on the\n",
    "        size of the current layer and the previous layer\n",
    "        Example: Size = [16,8,4,2]\n",
    "        The weight file will be a list with 3 matrices with shapes:\n",
    "        (8,16) for weights connecting layers 1 (16) and 2(8)\n",
    "        (4,8) for weights connecting layers 2 (8) and 4(4)\n",
    "        (2,4) for weights connecting layers 3 (4) and 4(2)\n",
    "        Each matrix will be initialized with random values\n",
    "        \"\"\"\n",
    "       #todo3-done\n",
    "        self.weights = [np.random.randn(self.size[i], self.size[i-1]) * np.sqrt(6 / self.size[i-1]+self.size[i]) for i in range(1, len(self.size))]\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Perform a feed forward computation\n",
    "        Parameters\n",
    "        ---\n",
    "        input: data to be fed to the network with\n",
    "        shape: (input_shape, batch_size)\n",
    "        Returns\n",
    "        ---\n",
    "        a: ouptut activation (output_shape, batch_size)\n",
    "        pre_activations: list of pre-activations per layer\n",
    "        each of shape (n[l], batch_size), where n[l] is the number\n",
    "        of neuron at layer l\n",
    "        activations: list of activations per layer\n",
    "        each of shape (n[l], batch_size), where n[l] is the number\n",
    "        of neuron at layer l\n",
    "        '''\n",
    "        a = input\n",
    "        pre_activations = []\n",
    "        activations = [a]\n",
    "        # TODO\n",
    "        #todo4 - done\n",
    "        # what does the zip function do?\n",
    "        #ans-zip is basically mapping similar index of multiple containers so that they can be used just using as single entity.\n",
    "        #so here updating the a by doing a dot product of activation of layer1 with the weights of connectins\n",
    "        #and adding a \"bias\" and then activating it by applying the sigmoid function to it.\n",
    "        #pre activation is the array of z before applying sigmoid function to it\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            #edit \n",
    "            w_shape=w.shape\n",
    "            a_shape=a.shape\n",
    "            if w_shape[1]!=a_shape[0]:\n",
    "                a=a.T\n",
    "            Z = np.dot(w, a) + b\n",
    "            a  = activation(Z)\n",
    "            pre_activations.append(Z)\n",
    "            activations.append(a)\n",
    "        return a, pre_activations, activations\n",
    "\n",
    "    \"\"\"\n",
    "    Resources:\n",
    "    https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "    https://hmkcode.github.io/ai/backpropagation-step-by-step/\n",
    "    \"\"\"\n",
    "    def compute_deltas(self, pre_activations, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes a list containing the values of delta for each layer using\n",
    "        a recursion\n",
    "        Parameters:\n",
    "        ---\n",
    "        pre_activations: list of of pre-activations. each corresponding to a layer\n",
    "        y_true: ground truth values of the labels\n",
    "        y_pred: prediction values of the labels\n",
    "        Returns:\n",
    "        ---\n",
    "        deltas: a list of deltas per layer\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize array to store the derivatives\n",
    "        deltas = [0] * (len(self.size) - 1)\n",
    "\n",
    "        #TODO\n",
    "        #todo5-done\n",
    "        # Calculate the delta for each layer\n",
    "        delta_layer=cost_function_prime(y_true, y_pred) * activation(pre_activations[-1], derivative=True)\n",
    "        # This is the first step in calculating the derivative\n",
    "        deltas[-1]=delta_layer\n",
    "        #The last layer is calculated as derivative of cost function *  derivative of sigmoid ( pre-activations of last layer )\n",
    "        #delta[-1] =\n",
    "\n",
    "        #TODO\n",
    "        #todo6-done\n",
    "        # Recursively calculate delta for each layer from the previous layer\n",
    "        for l in range(len(deltas) - 2, -1, -1):\n",
    "            pass\n",
    "            delta = np.dot(self.weights[l + 1].transpose(), deltas[l + 1]) * activation(pre_activations[l], derivative=True) \n",
    "            deltas[l] = delta\n",
    "            # deltas of layer l depend on the weights of layer l and l+1 and on the sigmoid derivative of the pre-activations of layer l\n",
    "            # Note that we use a dot product when multipying the weights and the deltas\n",
    "            # Check their shapes to ensure that their shapes conform to the requiremnts (You may need to transpose some of the matrices)\n",
    "            # The final shape of deltas of layer l must be the same as that of the activations of layer l\n",
    "            # Check if this is true\n",
    "            # delta[l] =\n",
    "        return deltas\n",
    "\n",
    "    def backpropagate(self, deltas, pre_activations, activations):\n",
    "        \"\"\"\n",
    "        Applies back-propagation and computes the gradient of the loss\n",
    "        w.r.t the weights and biases of the network\n",
    "        Parameters:\n",
    "        ---\n",
    "        deltas: list of deltas computed by compute_deltas\n",
    "        pre_activations: a list of pre-activations per layer\n",
    "        activations: a list of activations per layer\n",
    "        Returns:\n",
    "        ---\n",
    "        dW: list of gradients w.r.t. the weight matrices of the network\n",
    "        db: list of gradients w.r.t. the biases (vectors) of the network\n",
    "        \"\"\"\n",
    "        dW = []\n",
    "        db = []\n",
    "        deltas = [0] + deltas\n",
    "        for l in range(1, len(self.size)):\n",
    "            #edit\n",
    "            if deltas[l].shape[1] != activations[l-1].shape[0]:\n",
    "                activations[l-1]=activations[l-1].T\n",
    "            # TODO\n",
    "            #todo-7 done\n",
    "            q=activations[l-1]\n",
    "            dW_temp = np.dot(deltas[l],q ) \n",
    "            db_temp = deltas[l]\n",
    "            \n",
    "            # Compute the derivatives of the weights and the biases from the delta values calculated earlier\n",
    "            # dW_temp depends on the activations of layer l-1 and the deltas of layer l\n",
    "            # dB_temp depends only on the deltas of layer l\n",
    "            # Again be careful of the dimensions and ensure that the dW matrix has the same shape as W\n",
    "            # dW =\n",
    "            # dB =\n",
    "            dW.append(dW_temp)\n",
    "            db.append(np.expand_dims(db_temp.mean(axis=1), 1))\n",
    "        return dW, db\n",
    "\n",
    "    def plot_loss(self,epochs,train,test):\n",
    "        \"\"\"\n",
    "        Plots the loss function of the train test data measured every epoch\n",
    "        Parameters:\n",
    "        ---\n",
    "        epochs: number of epochs for training\n",
    "        train: list of losses on the train set measured every epoch\n",
    "        test: list of losses on the test set measured every epoch\n",
    "        \"\"\"\n",
    "\n",
    "        plt.subplot(211)\n",
    "        plt.title('Training Cost (loss)')\n",
    "        plt.plot(range(epochs),train)\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.title('Test Cost (loss)')\n",
    "        plt.plot(range(epochs),test)\n",
    "\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, X, y_org, batch_size, epochs, learning_rate, validation_split=0.2, print_every=10):\n",
    "        \"\"\"\n",
    "        Trains the network using the gradients computed by back-propagation\n",
    "        Splits the data in train and validation splits\n",
    "        Processes the training data by batches and trains the network using batch gradient descent\n",
    "        Parameters:\n",
    "        ---\n",
    "        X: input data\n",
    "        y: input labels\n",
    "        batch_size: number of data points to process in each batch\n",
    "        epochs: number of epochs for the training\n",
    "        learning_rate: value of the learning rate\n",
    "        validation_split: percentage of the data for validation\n",
    "        print_every: the number of epochs by which the network logs the loss and accuracy metrics for train and validations splits\n",
    "        plot_every: the number of epochs by which the network plots the decision boundary\n",
    "        Returns:\n",
    "        ---\n",
    "        history: dictionary of train and validation metrics per epoch\n",
    "            train_acc: train accuracy\n",
    "            test_acc: validation accuracy\n",
    "            train_loss: train loss\n",
    "            test_loss: validation loss\n",
    "        This history is used to plot the performance of the model\n",
    "        \"\"\"\n",
    "        temp_y=np.copy(y_org)\n",
    "        history_train_losses = []\n",
    "        history_train_accuracies = []\n",
    "        history_test_losses = []\n",
    "        history_test_accuracies = []\n",
    "        s=temp_y.tolist()\n",
    "        #one_hot encoding\n",
    "        for u in range(len(temp_y)):\n",
    "            if s[u]==0:\n",
    "                s[u]=[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            if s[u]==1:\n",
    "                s[u]=[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            if s[u]==2:\n",
    "                s[u]=[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "            if s[u]==3:\n",
    "                s[u]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "            if s[u]==4:\n",
    "                s[u]=[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "            if s[u]==5:\n",
    "                s[u]=[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "            if s[u]==6:\n",
    "                s[u]=[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "            if s[u]==7:\n",
    "                s[u]=[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
    "            if s[u]==8:\n",
    "                s[u]=[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "            if s[u]==9:\n",
    "                s[u]=[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        y=np.array(s)\n",
    "        \n",
    "\n",
    "        # TODO\n",
    "        #todo8- done\n",
    "        # Read about the train_test_split function\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "        epoch_iterator = range(epochs)\n",
    "\n",
    "        for e in epoch_iterator:\n",
    "            if x_train.shape[1] % batch_size == 0:\n",
    "                n_batches = int(x_train.shape[1] / batch_size)\n",
    "            else:\n",
    "                n_batches = int(x_train.shape[1] / batch_size ) - 1\n",
    "\n",
    "            x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "            batches_x = [x_train[batch_size*i:batch_size*(i+1),:] for i in range(0, n_batches)]\n",
    "            batches_y = [y_train[batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
    "\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "\n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "\n",
    "            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n",
    "            db_per_epoch = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            for batch_x, batch_y in zip(batches_x, batches_y):\n",
    "                batch_y_pred, pre_activations, activations = self.forward(batch_x)\n",
    "                deltas = self.compute_deltas(pre_activations, batch_y, batch_y_pred)\n",
    "                dW, db = self.backpropagate(deltas, pre_activations, activations)\n",
    "                for i, (dw_i, db_i) in enumerate(zip(dW, db)):\n",
    "                    dw_per_epoch[i] += dw_i / batch_size\n",
    "                    db_per_epoch[i] += db_i / batch_size\n",
    "\n",
    "                batch_y_train_pred = self.predict(batch_x)\n",
    "\n",
    "                train_loss = cost_function(batch_y, batch_y_train_pred)\n",
    "                train_losses.append(train_loss)\n",
    "                #edit\n",
    "                if batch_y.shape != batch_y_train_pred.shape:\n",
    "                    batch_y_train_pred = batch_y_train_pred.T\n",
    "                train_accuracy = accuracy_score(batch_y.T, batch_y_train_pred.T)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "\n",
    "                batch_y_test_pred = self.predict(x_test)\n",
    "\n",
    "                test_loss = cost_function(y_test, batch_y_test_pred)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracy = accuracy_score(y_test, batch_y_test_pred.T)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "\n",
    "\n",
    "            # weight update\n",
    "\n",
    "            # TODO\n",
    "            #todo9-done\n",
    "            # What does the enumerate function do?\n",
    "            #enumerate function sort of indexes the for loop means it adds a counter to an iterable and returns it in a form of enumerate object. \n",
    "            for i, (dw_epoch, db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n",
    "                pass\n",
    "                # TODO\n",
    "                #todo10-done\n",
    "                for i, (dw_epoch, db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n",
    "                    self.weights[i] = self.weights[i] - learning_rate * dw_epoch\n",
    "                    self.biases[i] = self.biases[i] - learning_rate * db_epoch\n",
    "                # Update the weights using the backpropagation algorithm implemented earlier\n",
    "                # W = W - learning_rate * derivatives (dW)\n",
    "                # b = b - learning_rate * derivatives (db)\n",
    "                # self.weights =\n",
    "                # self.biases =\n",
    "\n",
    "            history_train_losses.append(np.mean(train_losses))\n",
    "            history_train_accuracies.append(np.mean(train_accuracies))\n",
    "\n",
    "            history_test_losses.append(np.mean(test_losses))\n",
    "            history_test_accuracies.append(np.mean(test_accuracies))\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            if not plot_every:\n",
    "                if e % print_every == 0:    \n",
    "                    print('Epoch {} / {} | train loss: {} | train accuracy: {} | val loss : {} | val accuracy : {} '.format(\n",
    "                        e, epochs, np.round(np.mean(train_losses), 3), np.round(np.mean(train_accuracies), 3), \n",
    "                        np.round(np.mean(test_losses), 3),  np.round(np.mean(test_accuracies), 3)))\n",
    "            else:\n",
    "                if e % plot_every == 0:\n",
    "                    self.plot_decision_regions(x_train, y_train, e, \n",
    "                                                np.round(np.mean(train_losses), 4), \n",
    "                                                np.round(np.mean(test_losses), 4),\n",
    "                                                np.round(np.mean(train_accuracies), 4), \n",
    "                                                np.round(np.mean(test_accuracies), 4), \n",
    "                                                )\n",
    "                    plt.show()                    \n",
    "                    display.display(plt.gcf())\n",
    "                    display.clear_output(wait=True)\n",
    "\n",
    "        self.plot_decision_regions(X, y, e, \n",
    "                                    np.round(np.mean(train_losses), 4), \n",
    "                                    np.round(np.mean(test_losses), 4),\n",
    "                                    np.round(np.mean(train_accuracies), 4), \n",
    "                                    np.round(np.mean(test_accuracies), 4), \n",
    "                                    )\n",
    "            \"\"\"\n",
    "\n",
    "        history = {'epochs': epochs,\n",
    "                   'train_loss': history_train_losses,\n",
    "                   'train_acc': history_train_accuracies,\n",
    "                   'test_loss': history_test_losses,\n",
    "                   'test_acc': history_test_accuracies\n",
    "                   }\n",
    "        return history\n",
    "\n",
    "    def predict(self, a):\n",
    "        '''\n",
    "        Use the current state of the network to make predictions\n",
    "        Parameters:\n",
    "        ---\n",
    "        a: input data, shape: (input_shape, batch_size)\n",
    "        Returns:\n",
    "        ---\n",
    "        predictions: vector of output predictions\n",
    "        '''\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            #edit\n",
    "            if w.shape[1]!=a.shape[0]:\n",
    "                a=a.T\n",
    "            z = np.dot(w, a) + b\n",
    "            a = activation(z)\n",
    "        predictions = (a > 0.5).astype(int)\n",
    "        return predictions\n",
    "\n",
    "# Author: Ahmed BESBES\n",
    "# <ahmed.besbes@hotmail.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mnist.loader import MNIST\n",
    "\n",
    "mnist = MNIST(r\"C:/Users/ritik/Downloads/dataset/MNIST\")\n",
    "x_train, y_train = mnist.load_training() #60000 samples\n",
    "x_test, y_test = mnist.load_testing() #10000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  t10k-images-idx3-ubyte\n",
      "Reading  t10k-labels-idx1-ubyte\n",
      "Reading  train-images-idx3-ubyte\n",
      "Reading  train-labels-idx1-ubyte\n"
     ]
    }
   ],
   "source": [
    "import os,codecs,numpy\n",
    "\n",
    "# PROVIDE YOUR DIRECTORY WITH THE EXTRACTED FILES HERE\n",
    "datapath = 'C:/Users/ritik/Downloads/dataset/MNIST/'\n",
    "\n",
    "files = os.listdir(datapath)\n",
    "\n",
    "def get_int(b):   # CONVERTS 4 BYTES TO A INT\n",
    "    return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "data_dict = {}\n",
    "for file in files:\n",
    "    if file.endswith('ubyte'):  # FOR ALL 'ubyte' FILES\n",
    "        print('Reading ',file)\n",
    "        with open (datapath+file,'rb') as f:\n",
    "            data = f.read()\n",
    "            type = get_int(data[:4])   # 0-3: THE MAGIC NUMBER TO WHETHER IMAGE OR LABEL\n",
    "            length = get_int(data[4:8])  # 4-7: LENGTH OF THE ARRAY  (DIMENSION 0)\n",
    "            if (type == 2051):\n",
    "                category = 'images'\n",
    "                num_rows = get_int(data[8:12])  # NUMBER OF ROWS  (DIMENSION 1)\n",
    "                num_cols = get_int(data[12:16])  # NUMBER OF COLUMNS  (DIMENSION 2)\n",
    "                parsed = numpy.frombuffer(data,dtype = numpy.uint8, offset = 16)  # READ THE PIXEL VALUES AS INTEGERS\n",
    "                parsed = parsed.reshape(length,num_rows*num_cols)  # RESHAPE THE ARRAY AS [NO_OF_SAMPLES x HEIGHT x WIDTH]           \n",
    "            elif(type == 2049):\n",
    "                category = 'labels'\n",
    "                parsed = numpy.frombuffer(data, dtype=numpy.uint8, offset=8) # READ THE LABEL VALUES AS INTEGERS\n",
    "                parsed = parsed.reshape(length)  # RESHAPE THE ARRAY AS [NO_OF_SAMPLES]                           \n",
    "            if (length==10000):\n",
    "                set = 'test1'\n",
    "            elif (length==60000):\n",
    "                set = 'train1'\n",
    "            data_dict[set+'_'+category] = parsed  # SAVE THE NUMPY ARRAY TO A CORRESPONDING KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX = dataset[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn X, y\n",
    "def generate_fake_samples(n_samples):\n",
    "\t# generate uniform random numbers in [0,1]\n",
    "\tX = numpy.random.rand(28 * 28 * n_samples)\n",
    "\t# reshape into a batch of grayscale images\n",
    "\tX = X.reshape((n_samples, 28, 28, 1))\n",
    "\t# generate 'fake' class labels (0)\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 0., 4., ..., 5., 6., 8.], dtype=float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_training1= data_dict['train1_images']\n",
    "x_training1.astype(\"float16\")\n",
    "y_training1= data_dict['train1_labels']\n",
    "y_training1.astype(\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 784)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "x_training2=[]\n",
    "y_training2=[]\n",
    "for t in range(500):\n",
    "    x_training2.append(x_training1[t]/255)\n",
    "for t in range(500):\n",
    "    y_training2.append(y_training1[t])    \n",
    "x_training2=np.array(x_training2) \n",
    "y_training2=np.array(y_training2) \n",
    "print (x_training2.shape)\n",
    "print (y_training2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:288: RuntimeWarning: Mean of empty slice.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-16b001ff999b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_training2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_training2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-7d9553cdd5d3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y_org, batch_size, epochs, learning_rate, validation_split, print_every)\u001b[0m\n\u001b[0;32m    407\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mbatch_y_train_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                     \u001b[0mbatch_y_train_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_y_train_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                 \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y_train_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m                 \u001b[0mtrain_accuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multilabel-indicator\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: unknown is not supported"
     ]
    }
   ],
   "source": [
    "nn=NeuralNetwork([784, 300, 10, 10])\n",
    "nn.train(x_training2, y_training2, 10, 15,  0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "X = numpy.random.rand(28 * 28 * 2)\n",
    "# reshape into a batch of grayscale images\n",
    "X = X.reshape((2, 28, 28, 1))\n",
    "for i in range(2):\n",
    "    img = X[i].reshape((28,28))\n",
    "    plt.imshow(img, cmap=\"Greys\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
